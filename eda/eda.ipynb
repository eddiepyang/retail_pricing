{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# algebra and dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# text processing\n",
    "import re\n",
    "import spacy\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# hypothesis testing\n",
    "from scipy.stats import gamma, kstest, lognorm, mannwhitneyu, ks_2samp\n",
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "# charts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data processing\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline, make_union, FeatureUnion\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# measurement metrics\n",
    "from sklearn.metrics import classification_report, make_scorer, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "# charting paramemters\n",
    "from pylab import rcParams\n",
    "rcParams.update({'font.size' : 14, 'legend.fontsize' : \"small\", \n",
    "                 \"xtick.labelsize\" : 14, \"ytick.labelsize\" : 14, \n",
    "                 \"figure.figsize\":(9, 6), \"axes.titlesize\" : 20,\n",
    "                 \"axes.labelsize\" : 14, \"lines.linewidth\" : 3, \n",
    "                 \"lines.markersize\" : 10\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table('../data/train.tsv')\n",
    "test = pd.read_table('../data/test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick = True\n",
    "\n",
    "if quick:\n",
    "    data = data.sample(frac=.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 148254 entries, 1065093 to 786483\n",
      "Data columns (total 8 columns):\n",
      "train_id             148254 non-null int64\n",
      "name                 148254 non-null object\n",
      "item_condition_id    148254 non-null int64\n",
      "category_name        147588 non-null object\n",
      "brand_name           84961 non-null object\n",
      "price                148254 non-null float64\n",
      "shipping             148254 non-null int64\n",
      "item_description     148254 non-null object\n",
      "dtypes: float64(1), int64(3), object(4)\n",
      "memory usage: 10.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1065093</th>\n",
       "      <td>1065093</td>\n",
       "      <td>Kylie Metal Matte Kymajesty</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty/Makeup/Lips</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new in box - Never Swatched! Kylie's exo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407370</th>\n",
       "      <td>407370</td>\n",
       "      <td>Joggers camouflage</td>\n",
       "      <td>2</td>\n",
       "      <td>Kids/Boys (4+)/Bottoms</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Camouflage jogger pants never worn at all boy'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688213</th>\n",
       "      <td>688213</td>\n",
       "      <td>LuLaRoe Cassie &amp; Irma Bundle - XL/Medium</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Skirts/Straight, Pencil</td>\n",
       "      <td>LuLaRoe</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This gorgeous Cassie has a solid black backgro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155188</th>\n",
       "      <td>155188</td>\n",
       "      <td>Cute Sequins Top</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Maurices</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Excellent condition except a lil highlighter m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309673</th>\n",
       "      <td>1309673</td>\n",
       "      <td>NEW NYX eyeshadow palette avant pop</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty/Makeup/Makeup Palettes</td>\n",
       "      <td>NYX</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Brand new and sealed NYX eyeshadow palette in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565424</th>\n",
       "      <td>565424</td>\n",
       "      <td>Deadrising 3 and a 3 day Xbox live card</td>\n",
       "      <td>3</td>\n",
       "      <td>Vintage &amp; Collectibles/Electronics/Video Game</td>\n",
       "      <td>Xbox</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>It's in a pretty good condition and used but w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043925</th>\n",
       "      <td>1043925</td>\n",
       "      <td>FULL ZIP HOODIE</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Sweaters/Full Zip</td>\n",
       "      <td>PINK</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>BRAND NEW VS PINK FULL ZIP HOODIE PRICE IS FIRM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309082</th>\n",
       "      <td>1309082</td>\n",
       "      <td>9months boy</td>\n",
       "      <td>3</td>\n",
       "      <td>Kids/Boys 0-24 Mos/One-Pieces</td>\n",
       "      <td>Carter's</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Baby boy 9m cloths . No stains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966563</th>\n",
       "      <td>966563</td>\n",
       "      <td>Black Nike pro spandex shorts size XL</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Athletic Apparel/Shorts</td>\n",
       "      <td>Nike</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Brand new with tags Black Nike pro spandex sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428151</th>\n",
       "      <td>1428151</td>\n",
       "      <td>Rae Dunn Faith Mug And Blessed Bowl</td>\n",
       "      <td>1</td>\n",
       "      <td>Home/Kitchen &amp; Dining/Coffee &amp; Tea Accessories</td>\n",
       "      <td>Rae Dunn</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rae Dunn Faith mug Blessed bowl bundle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id                                      name  \\\n",
       "1065093   1065093               Kylie Metal Matte Kymajesty   \n",
       "407370     407370                        Joggers camouflage   \n",
       "688213     688213  LuLaRoe Cassie & Irma Bundle - XL/Medium   \n",
       "155188     155188                          Cute Sequins Top   \n",
       "1309673   1309673       NEW NYX eyeshadow palette avant pop   \n",
       "565424     565424   Deadrising 3 and a 3 day Xbox live card   \n",
       "1043925   1043925                           FULL ZIP HOODIE   \n",
       "1309082   1309082                               9months boy   \n",
       "966563     966563     Black Nike pro spandex shorts size XL   \n",
       "1428151   1428151       Rae Dunn Faith Mug And Blessed Bowl   \n",
       "\n",
       "         item_condition_id                                   category_name  \\\n",
       "1065093                  1                              Beauty/Makeup/Lips   \n",
       "407370                   2                          Kids/Boys (4+)/Bottoms   \n",
       "688213                   1                   Women/Skirts/Straight, Pencil   \n",
       "155188                   3                     Women/Tops & Blouses/Blouse   \n",
       "1309673                  1                   Beauty/Makeup/Makeup Palettes   \n",
       "565424                   3   Vintage & Collectibles/Electronics/Video Game   \n",
       "1043925                  1                         Women/Sweaters/Full Zip   \n",
       "1309082                  3                   Kids/Boys 0-24 Mos/One-Pieces   \n",
       "966563                   1                   Women/Athletic Apparel/Shorts   \n",
       "1428151                  1  Home/Kitchen & Dining/Coffee & Tea Accessories   \n",
       "\n",
       "        brand_name  price  shipping  \\\n",
       "1065093        NaN   18.0         1   \n",
       "407370     Arizona    9.0         0   \n",
       "688213     LuLaRoe   54.0         1   \n",
       "155188    Maurices    6.0         1   \n",
       "1309673        NYX   10.0         1   \n",
       "565424        Xbox   14.0         0   \n",
       "1043925       PINK   49.0         0   \n",
       "1309082   Carter's   21.0         0   \n",
       "966563        Nike   24.0         0   \n",
       "1428151   Rae Dunn   26.0         0   \n",
       "\n",
       "                                          item_description  \n",
       "1065093  Brand new in box - Never Swatched! Kylie's exo...  \n",
       "407370   Camouflage jogger pants never worn at all boy'...  \n",
       "688213   This gorgeous Cassie has a solid black backgro...  \n",
       "155188   Excellent condition except a lil highlighter m...  \n",
       "1309673  Brand new and sealed NYX eyeshadow palette in ...  \n",
       "565424   It's in a pretty good condition and used but w...  \n",
       "1043925    BRAND NEW VS PINK FULL ZIP HOODIE PRICE IS FIRM  \n",
       "1309082                     Baby boy 9m cloths . No stains  \n",
       "966563   Brand new with tags Black Nike pro spandex sho...  \n",
       "1428151             Rae Dunn Faith mug Blessed bowl bundle  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>shipping</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>32.744244</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.445607</td>\n",
       "      <td>14.0</td>\n",
       "      <td>35924.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>29.063278</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.734701</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>27.993743</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.061085</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>27.108725</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.142433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1011.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>0</th>\n",
       "      <td>33.615894</td>\n",
       "      <td>21.0</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.113636</td>\n",
       "      <td>15.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 mean  median      len\n",
       "item_condition_id shipping                            \n",
       "1                 0         32.744244    23.0  28101.0\n",
       "                  1         21.445607    14.0  35924.0\n",
       "2                 0         29.063278    19.0  23294.0\n",
       "                  1         23.734701    15.0  14282.0\n",
       "3                 0         27.993743    18.0  28287.0\n",
       "                  1         24.061085    15.0  14881.0\n",
       "4                 0         27.108725    16.0   2235.0\n",
       "                  1         20.142433    12.0   1011.0\n",
       "5                 0         33.615894    21.0    151.0\n",
       "                  1         31.113636    15.0     88.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_price = data.groupby(['item_condition_id', 'shipping'])['price'].agg([np.mean, np.median, len])\n",
    "tbl_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.category_name.value_counts() > 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean', 'median', 'len'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_price.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex(levels=[[1, 2, 3, 4, 5], [0, 1]],\n",
       "           labels=[[0, 0, 1, 1, 2, 2, 3, 3, 4, 4], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]],\n",
       "           names=['item_condition_id', 'shipping'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_price.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean         27.993743\n",
       "median       18.000000\n",
       "len       28287.000000\n",
       "Name: (3, 0), dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_price.loc[(3,0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_condition_id\n",
       "1    3095\n",
       "2    1449\n",
       "3    1312\n",
       "4      61\n",
       "5       1\n",
       "Name: price, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.category_name=='Women/Athletic Apparel/Pants, Tights, Leggings'].groupby('item_condition_id')['price'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_condition_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.654766</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.806073</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.676067</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1312.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.721311</td>\n",
       "      <td>14.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mean  median     len\n",
       "item_condition_id                           \n",
       "1                  38.654766    35.0  3095.0\n",
       "2                  30.806073    25.0  1449.0\n",
       "3                  26.676067    20.0  1312.0\n",
       "4                  19.721311    14.0    61.0\n",
       "5                   4.000000     4.0     1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.category_name=='Women/Athletic Apparel/Pants, Tights, Leggings'].groupby('item_condition_id')['price'].agg([np.mean, np.median, len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer( max_df = 0.99, min_df = 10, stop_words = 'english')\n",
    "vectorizer2 = TfidfVectorizer(max_df = 0.2, min_df = 10, stop_words = 'english')\n",
    "# %%time\n",
    "nmf = NMF(n_components = 20, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_len(cell):\n",
    "    try: \n",
    "        if np.isnan(cell):\n",
    "            return 0\n",
    "    except:\n",
    "        return len(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['desc_lengths'] = [count_len(item) for item in data.item_description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very durable and comfortable New!'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "s = data.item_description.iloc[55]+'!'\n",
    "s.translate(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable = ['ner'])\n",
    "\n",
    "def regex_replace(texts, substitute = '', regex_pattern = r\"[^a-zA-z' ]|'s\" ):\n",
    "    \n",
    "    pattern = re.compile(regex_pattern)\n",
    "    \n",
    "    result = []\n",
    "    for text in texts:\n",
    "        replaced = pattern.sub(substitute, text)\n",
    "        replaced = replaced.replace(r\"\\n\", '').replace('  ', ' ').lower().strip()\n",
    "        result.append(replaced)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def join_not(text):\n",
    "    replace_these = re.findall(r'not\\s+\\w+', text)\n",
    "    for item in replace_these:\n",
    "        tmp = item.replace(' ', '_')\n",
    "        text = text.replace(item, tmp)\n",
    "\n",
    "    return text\n",
    "\n",
    "def reg_replace(text):\n",
    "        # remove HTML\n",
    "    # text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    #removes remaining urls\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text, flags=re.MULTILINE)\n",
    "       \n",
    "    # removes numbers, symbols, nextline symbols \n",
    "    text = re.sub(r\"[^a-zA-Z' ]\",\" \", text)   \n",
    "    text = text.replace(r\"\\n\", '').replace('  ', ' ').lower().strip('.')\n",
    "    \n",
    "    # removes some possessive, needs improvement\n",
    "    text = re.sub(r\"'s\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text, join = False):\n",
    "    \n",
    "    #text = text.replace(r\"\\n\", '').replace('  ', ' ').lower()\n",
    "    \n",
    "    if join:\n",
    "        text = join_not(text)\n",
    "        \n",
    "    text = nlp(texts)\n",
    "    \n",
    "    # removes punctuation and pronouns, random words, normalizes words by lemmatization\n",
    "    words=[]\n",
    "    for word in text:\n",
    "        if word.pos_ != 'PUNCT' and word.lemma_ != '-PRON-' and not word.is_space:\n",
    "            if word not in {\"x\", \"s\", \"v\", \"'s\"}:\n",
    "                words.append(word.lemma_)\n",
    "\n",
    "                \n",
    "def clean_text2(texts):\n",
    "    \n",
    "    texts = nlp.pipe(texts, batch_size = 1000)\n",
    "    \n",
    "    # removes punctuation and pronouns, random words, normalizes words by lemmatization\n",
    "    word_lists=[]\n",
    "    for sent in texts:\n",
    "        words = []\n",
    "        for word in sent:\n",
    "            if (word.pos_ != 'PUNCT' and word.lemma_ != '-PRON-') and \\\n",
    "            (not word.is_space and not word.is_stop):\n",
    "                if word.lemma_ not in {\"x\", \"s\", \"v\", \"'s\"}:\n",
    "                    words.append(word.lemma_)\n",
    "        word_lists.append(words)\n",
    "                \n",
    "    return word_lists\n",
    "\n",
    "def blob_lemmatize(text):\n",
    "    \n",
    "    text = TextBlob(text)\n",
    "    word_list = []\n",
    "    for word in text.words:\n",
    "        word_list.append(word.lemmatize('v'))\n",
    "    return word_list\n",
    "\n",
    "def remove_stops(unigrams, removals=['not'], join = False):\n",
    "    \n",
    "    # removes item from set\n",
    "    if len(removals) > 0:\n",
    "        for item in removals:\n",
    "            try:\n",
    "                spacy.lang.en.STOP_WORDS.remove(item)\n",
    "            except Exception as error:\n",
    "                #print(error)\n",
    "                pass\n",
    "    \n",
    "    unigrams = [word for word in unigrams if word not in spacy.lang.en.STOP_WORDS]\n",
    "    \n",
    "    if join:\n",
    "        return ' '.join(unigrams)\n",
    "    else:\n",
    "        return unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'would' in spacy.lang.en.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_list = regex_replace(data.item_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ryeyoo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ryeyoo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 58.6 ms, total: 1min 15s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "descs = [blob_clean(item) for item in desc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41min 30s, sys: 1h 5min 51s, total: 1h 47min 22s\n",
      "Wall time: 9min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#desc_list = regex_replace(data.item_description)\n",
    "item_descriptions = clean_text2(desc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 s, sys: 40.1 ms, total: 29.5 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "phrase_model = Phraser(Phrases(item_descriptions))\n",
    "parsed_bigrams = [' '.join(phrase_model[item_descriptions[i]]) for i in range(0,len(item_descriptions))]\n",
    "description_matrix = vectorizer.fit_transform(parsed_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_select(data):\n",
    "    return data['item_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = make_pipeline(FunctionTransformer(func = column_select, validate = False), vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=10,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...iter=200,\n",
       "  n_components=20, random_state=20, shuffle=False, solver='cd', tol=0.0001,\n",
       "  verbose=0))])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline([('vec', vectorizer), ('nmf', nmf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = FeatureUnion(transformer_list = \n",
    "    [ \n",
    "        \n",
    "    ('nmf_pipe', Pipeline([('vec', vectorizer), ('nmf', nmf)])),\n",
    "    ('svd_pipe', Pipeline([('vec', vectorizer), ('svd', svd)]))\n",
    "    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_pipe = FeatureUnion(transformer_list = \n",
    "    [ \n",
    "        \n",
    "    ('nmf_pipe', make_pipeline(ItemSelector('item_description'), vectorizer, nmf)),\n",
    "    ('svd_pipe', make_pipeline(ItemSelector('item_description'), vectorizer, svd)),\n",
    "    ('shipping + condition', make_pipeline(ItemSelector(['shipping', 'item_condition_id']))),    \n",
    "    ('target_encode', make_pipeline(ItemSelector(['category_name', 'brand_name']), ce.target_encoder.TargetEncoder(return_df = False)))\n",
    "    \n",
    "    ])\n",
    "\n",
    "\n",
    "final_pipe = make_pipeline(desc_pipe, RobustScaler(), xgb.XGBRegressor(n_jobs = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard scaler: 30.828372, 105.044950\n"
     ]
    }
   ],
   "source": [
    "print('standard scaler: %f, %f' % (np.sqrt(np.mean(-cv_score)), np.std(cv_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['parsed_descriptions'] = parsed_bigrams\n",
    "cols = [item for item in data.columns.values if item not in ['price', 'names', 'train_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.828371814228504"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean(-cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.828377385531592"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean(-cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':250}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=4, min_child_weight=1, missing=None, n_estimators=250,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "def rmsle(y, pred_y):\n",
    "    \n",
    "    pred_y = np.fmax(pred_y, 0)\n",
    "    rmsle = 1/len(y) * np.sqrt(np.mean(np.log(1 + pred_y) - np.log(1 + y)))\n",
    "    return rmsle\n",
    "\n",
    "def function(params = {'n_jobs': 12}):\n",
    "    \n",
    "    print(params)\n",
    "    print('start time %s' % time.ctime())\n",
    "    feature_pipe = FeatureUnion(transformer_list = \n",
    "    [ \n",
    "        \n",
    "    ('nmf_pipe', make_pipeline(ItemSelector('item_description'), vectorizer, nmf)),\n",
    "    ('svd_pipe', make_pipeline(ItemSelector('item_description'), vectorizer, svd)),\n",
    "    ('shipping + condition', make_pipeline(ItemSelector(['shipping', 'item_condition_id']))),    \n",
    "    ('target_encode', make_pipeline(ItemSelector(['category_name', 'brand_name']), ce.target_encoder.TargetEncoder(return_df = False)))\n",
    "    \n",
    "    ])\n",
    "\n",
    "    final_pipe = make_pipeline(feature_pipe, RobustScaler(), XGBRegressor(**params))\n",
    "\n",
    "    cv_score = cross_val_score(final_pipe, data, data['price'], cv = 5, scoring = make_scorer(rmsle))\n",
    "    \n",
    "    # displays time for optimization cycle purposes\n",
    "    print(time.ctime())\n",
    "    \n",
    "    \n",
    "    print(np.mean(final_score))\n",
    "    \n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.item_description = data.item_description.fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_jobs': 12}\n",
      "start time Sun Aug 19 18:17:53 2018\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        'max_depth': hp.choice('max_depth', np.arange(10, 30, dtype=int)),\n",
    "        'min_child_weight': hp.quniform ('min_child', 1, 20, 1),\n",
    "        'subsample': hp.uniform ('subsample', 0.8, 1),\n",
    "        'n_estimators' : hp.choice('n_estimators', np.arange(1000, 10000, 100, dtype=int)),\n",
    "        'learning_rate' : hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n",
    "        'gamma' : hp.quniform('gamma', 0.1, 1, 0.05),\n",
    "        'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.9, 'learning_rate': 0.375, 'max_depth': 3, 'min_child_weight': 3.0, 'n_estimators': 200, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 14:47:31 2018\n",
      "31.06589939527867\n",
      "{'gamma': 0.1, 'learning_rate': 0.42500000000000004, 'max_depth': 8, 'min_child_weight': 1.0, 'n_estimators': 250, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 14:57:17 2018\n",
      "32.32725717423393\n",
      "{'gamma': 1.0, 'learning_rate': 0.25, 'max_depth': 6, 'min_child_weight': 5.0, 'n_estimators': 250, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 15:06:32 2018\n",
      "31.07221629528323\n",
      "{'gamma': 0.4, 'learning_rate': 0.45, 'max_depth': 2, 'min_child_weight': 6.0, 'n_estimators': 100, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 15:14:41 2018\n",
      "30.934071036453478\n",
      "{'gamma': 0.7000000000000001, 'learning_rate': 0.45, 'max_depth': 6, 'min_child_weight': 5.0, 'n_estimators': 200, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 15:23:43 2018\n",
      "32.28858308179426\n",
      "{'gamma': 0.4, 'learning_rate': 0.4, 'max_depth': 9, 'min_child_weight': 1.0, 'n_estimators': 250, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 15:33:49 2018\n",
      "32.67115739839979\n",
      "{'gamma': 0.15000000000000002, 'learning_rate': 0.275, 'max_depth': 6, 'min_child_weight': 4.0, 'n_estimators': 250, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 15:43:05 2018\n",
      "31.234624499539898\n",
      "{'gamma': 0.30000000000000004, 'learning_rate': 0.125, 'max_depth': 8, 'min_child_weight': 5.0, 'n_estimators': 100, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 15:51:46 2018\n",
      "30.511901789103526\n",
      "{'gamma': 0.8, 'learning_rate': 0.375, 'max_depth': 2, 'min_child_weight': 3.0, 'n_estimators': 200, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 16:00:02 2018\n",
      "30.969798933440927\n",
      "{'gamma': 0.65, 'learning_rate': 0.30000000000000004, 'max_depth': 2, 'min_child_weight': 3.0, 'n_estimators': 150, 'n_jobs': 12, 'random_state': 42}\n",
      "Sun Aug 19 16:08:15 2018\n",
      "30.90952917370619\n",
      "{'gamma': 0.30000000000000004, 'learning_rate': 0.125, 'max_depth': 6, 'min_child_weight': 5.0, 'n_estimators': 0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "best = fmin(\n",
    "    fn=function,\n",
    "    \n",
    "    space={\n",
    "        'n_estimators': hp.choice('n_estimators', np.arange(100, 300, 50)),\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.1, 0.5, 0.025),\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(2, 10, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "        'n_jobs': 12,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    \n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 10\n",
    "           )\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.30000000000000004,\n",
       " 'learning_rate': 0.125,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 5.0,\n",
       " 'n_estimators': 0}"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={\n",
    "        'n_estimators': hp.choice('n_estimators', np.arange(100, 300, 50)),\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.1, 0.5, 0.025),\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(2, 10, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "        'n_jobs': 12,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "\n",
    "from hyperopt import space_eval\n",
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.30000000000000004,\n",
       " 'learning_rate': 0.125,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 5.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': 12,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.875792741605146"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean(-cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.33422351942155"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean(-cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148254, 9)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=12,\n",
       "       transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=10,\n",
       "    ...runcatedSVD(algorithm='randomized', n_components=20, n_iter=5,\n",
       "       random_state=20, tol=0.0))]))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(parsed_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1065093    Brand new in box - Never Swatched! Kylie's exo...\n",
       "407370     Camouflage jogger pants never worn at all boy'...\n",
       "688213     This gorgeous Cassie has a solid black backgro...\n",
       "155188     Excellent condition except a lil highlighter m...\n",
       "1309673    Brand new and sealed NYX eyeshadow palette in ...\n",
       "565424     It's in a pretty good condition and used but w...\n",
       "1043925      BRAND NEW VS PINK FULL ZIP HOODIE PRICE IS FIRM\n",
       "1309082                       Baby boy 9m cloths . No stains\n",
       "966563     Brand new with tags Black Nike pro spandex sho...\n",
       "1428151               Rae Dunn Faith mug Blessed bowl bundle\n",
       "68447      Features: 100% Brand New. Material: Neoprene a...\n",
       "91265      Fenty flip flop! Super cozy, awesome color! Co...\n",
       "77999                                   Green Day band shirt\n",
       "975978     From ipsy bag new never used never swatched it...\n",
       "1066167    NWOT 2PC VS PINK V-NECK LS TEE AND PINK NATION...\n",
       "524248     Funko Pop! Jimi Hendrix exclusive figure with ...\n",
       "1151062    ☆PERFECT MOTHER'S DAY GIFT☆ ☆Includes 25w bulb...\n",
       "1068439    Only wore them and few time and they will come...\n",
       "604998                                            Brand new.\n",
       "451579     New in package Size large Oversized fit No fre...\n",
       "477755     Boutique brand Famosa dress. Size small but wo...\n",
       "573539     Adorable leggings with functional back pockets...\n",
       "363090     Faux LV shoulder bag Great condition never use...\n",
       "1010639    Brand new from online comes in bag as shown si...\n",
       "1016928    Free shipping Includes 3 carbonated Bubble Cla...\n",
       "353108                         From smoke free home 11\" tall\n",
       "1008561                                   No description yet\n",
       "423234         Caramel Apple Silver Glitter Gloss Sand Gloss\n",
       "781460     Victoria's Secret VS PINK Limited edition PINK...\n",
       "538909     Black relaxed tunic tee. Great for layering ov...\n",
       "                                 ...                        \n",
       "1292116    Topshop Stripe Print Collar Tee - size US 4/UK...\n",
       "1196862    This flowing rose colored knit top is perfect ...\n",
       "457735     Brand new Michael kors jet set large crossbody...\n",
       "1300435    Baseball jersey style from forever 21 size m. ...\n",
       "911678     Great for kids to play with. New never opened....\n",
       "888957     Hot Hot Shapers Belt, made of high-tech fiber ...\n",
       "621968     Medium! Worn once but it's too small on me per...\n",
       "108906                Black and Gray Cheetah print Converses\n",
       "78946      Beautiful ring! Bigger than it looks! Adjustab...\n",
       "346934     \"Are you kitten me\" Mocs Never worn So cute, p...\n",
       "1077163    Perfect condition. No flaws. Beautiful bra and...\n",
       "758695                          Great condition. Worn 1 time\n",
       "844704     Lululemon hoodie slight wash wear no rip tag s...\n",
       "548366                                     Size 7.5 in women\n",
       "1115421            Beautiful HTF Print. EUC. Worn only once.\n",
       "1200765    YOU WILL GET MARIO KART DS & MARIO AND SONIC A...\n",
       "72455                                      New sealed 1 inch\n",
       "86887                                     No description yet\n",
       "202258             Mermaid Lot (4) 14 G stainless steel post\n",
       "143945     These products are : Mac, Avon, Milani, Maybel...\n",
       "348649     TARTE Double Duty Beauty Shape Tape Contour Co...\n",
       "403463     BRAND NEW SEALED UNTOUCHED HAS 12 COLORS COMES...\n",
       "250556     Sticky Silicone Bra Adhesive Stick On Gel Push...\n",
       "199641     5in custom decal Larger decals are [rm] per in...\n",
       "294724                                      Size xs fits big\n",
       "885979     Brand new with tags never used Bag dimensions ...\n",
       "682701               Brand New with tags Original Price [rm]\n",
       "1418226    . Pop, tilt, wrap, prop, collapse, grip, repea...\n",
       "986990     SEE ALL PHOTOS PLEASE Has some scratches due t...\n",
       "786483      NWOT size 8, blue/ green stone. Stainless steel.\n",
       "Name: item_description, Length: 148254, dtype: object"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItemSelector('item_description').transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe1.transform(parsed_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148254, 40)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe = make_pipeline(pipe1, xgb.XGBRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-68d6f505245e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#make_union(make_pipeline(vectorizer, nmf), validate=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mFeatureUnion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpipe1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transformer_list, n_jobs, transformer_weights)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_validate_transformers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;31m# validate names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "#make_union(make_pipeline(vectorizer, nmf), validate=False)\n",
    "FeatureUnion([pipe1, xgb.XGBRegressor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 µs, sys: 5 µs, total: 36 µs\n",
      "Wall time: 41.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed = 20\n",
    "nmf = NMF(n_components = 20, random_state = seed)\n",
    "#description_W = nmf.fit_transform(description_matrix)\n",
    "\n",
    "svd = TruncatedSVD(n_components = 20, random_state = seed)\n",
    "#description_svd = svd.fit_transform(description_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(description_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idx, test_idx = train_test_split(range(len(data)), test_size = 0.3, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# from sklearn's tutorial\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "    \n",
    "class categorical_means(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None, col_name = 'category_name', response_var = 'price'):\n",
    "#         stuff = cat_means(X)\n",
    "        return self.cat_means(X, col_name, response_var)\n",
    "#         return stuff\n",
    "    \n",
    "    def tmp(self, data):\n",
    "        return data\n",
    "    \n",
    "    def cat_means(self, data, col_name, response_var):\n",
    "    \n",
    "        grouped_data = data.groupby(col_name)[response_var].agg([len, np.mean, np.median])\n",
    "        conversion_dict = {name:mean for name, mean in zip(grouped_data[grouped_data.len > 30].index, \n",
    "                                                       grouped_data[grouped_data.len > 30]['mean'])}\n",
    "        converted = [conversion_dict[name] if name in conversion_dict else -1 for name in data[col_name]]\n",
    "\n",
    "        return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = categorical_means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.transform(data, col_name = 'brand_name', response_var = 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pipeline(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category_means'] = cat_group_means(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped_data = data.groupby('brand_name')['price'].agg([len, np.mean, np.median])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_group_means(data, col_name = 'brand_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(description_W), pd.DataFrame(description_svd), data.reset_index(drop=True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(description_W), pd.DataFrame(description_svd), data.reset_index(drop=True)], axis = 1).to_csv('../processed/converted_array.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan('arg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['cleaned_brands'] = [clean_text(item) for item in data.brand_name.fillna('unknown')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict = {}\n",
    "for item, score in phrases:\n",
    "    bigram_dict[item] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity recognition example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"The White House climb-down from President Donald Trump’s disastrous news conference with his Russian counterpart Vladimir Putin began Monday night, just hours after Trump said he saw “no reason” why Russia would have meddled in the 2016 election.\n",
    "\n",
    "With even The Wall Street Journal’s editorial board — normally intensely loyal to Trump — joining in widespread criticism of the president’s implicit public rejection of U.S. intelligence claims, the White House circulated talking points to supporters saying that Trump still had great confidence in his intelligence agencies and that he believed their assessment that the Kremlin actively influenced the vote.\n",
    "\n",
    "But the president himself emerged on Tuesday to personally walk back his statements in Helsinki, using a scheduled meeting with members of Congress to discuss tax reform as a platform for revising the statements that set off a 24-hour firestorm.\n",
    "\n",
    "“In a key sentence in my remarks, I said the word ‘would’ instead of ‘wouldn’t,’” Trump said. “The sentence should have been — and I thought it would be maybe a little bit unclear on the transcript or unclear on the actual video — the sentence should have been: I don’t see any reason why it wouldn't be Russia. Sort of a double negative.”\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data.desc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?lognorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a gamma dist\n",
    "fit_alpha, fit_loc, fit_beta=gamma.fit(data[data.category_name=='Women/Athletic Apparel/Pants, Tights, Leggings'].price)\n",
    "print(fit_alpha, fit_loc, fit_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma.fit(list(data[data.category_name=='Women/Athletic Apparel/Pants, Tights, Leggings'].price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognorm._fitstart(data[data.category_name=='Women/Athletic Apparel/Pants, Tights, Leggings'].price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognorm.fit(data[data.category_name=='Women/Athletic Apparel/Pants, Tights, Leggings'].price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?lognorm.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?kstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_2samp(np.array(data.price[data.brand_name.isnull()]), np.array(data.price[data.brand_name.notnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-parametric indepedent samples\n",
    "mannwhitneyu(data.price[data.brand_name.isnull()], \n",
    "                       data.price[data.brand_name.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data[data.category_name=='Women/Athletic Apparel/Pants, Tights, Leggings'].price, bins = 100)\n",
    "plt.xlim(0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data, values = 'price', index = 'brand_name', columns = 'shipping', aggfunc=(np.median, len)).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple to select multi-level index\n",
    "pd.pivot_table(data, values = 'price', index = 'brand_name', columns = 'shipping', aggfunc=(np.median, len))[('len', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_tbl = pd.pivot_table(data, values = 'price', index = 'brand_name', aggfunc=('mean', np.median, len), dropna=False).sort_values('len', ascending=False)\n",
    "brand_tbl[brand_tbl.len > 30].sort_values('median', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.brand_name.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.brand_name.value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_proportion = data.brand_name.value_counts(dropna=False, normalize=True)\n",
    "1- data.brand_name.value_counts(dropna=False, normalize=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['brand_name_null'] = data.brand_name.isnull()*1\n",
    "sns.kdeplot(data.price[data.brand_name.notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(sharex=True, sharey=True)\n",
    "\n",
    "for data, label in zip([data.price[data.brand_name.isnull()], \n",
    "                       data.price[data.brand_name.notnull()]], \n",
    "                      [\"0\", \"1\"]):\n",
    "     sns.kdeplot(data,  ax=ax, label = label, shade = True)\n",
    "     #sns.distplot(data,  ax=ax, label = label)\n",
    "   \n",
    "#ax.set_xlim([-5, 250])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(data.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data.fillna(-1), values = 'price', index = 'brand_name', aggfunc=(np.median, len), dropna=False).sort_values('len', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picks categorical columns, check for diff datasets\n",
    "def get_categoricals(data):\n",
    "\n",
    "    categorical_columns = []\n",
    "    for dtype, idx in zip(data.dtypes, data.dtypes.index):\n",
    "        if dtype == 'object':\n",
    "            categorical_columns.append(idx)\n",
    "    return categorical_columns\n",
    "\n",
    "def encode_column(train_column, test_column, fillna = False):\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # le does not work with nan\n",
    "    train_column = train_column.fillna('unknown')\n",
    "    test_column = test_column.fillna('unknown')\n",
    "    le.fit(train_column)\n",
    "    train_column_le = le.transform(train_column)\n",
    "    test_column_le = le.transform(test_column)\n",
    "    translation_dict = {i:item for i, item in enumerate(le.classes_)}\n",
    "    return train_column_le, test_column_le, translation_dict\n",
    "\n",
    "# categorical to column arrays wide\n",
    "def label_convert(train_column, test_column):\n",
    "    \n",
    "    encoder = LabelBinarizer()\n",
    "    label_model = encoder.fit(train_column.fillna('unknown'))\n",
    "    converted_train = label_model.transform(train_column.fillna('unknown'))\n",
    "    converted_test = label_model.transform(test_column.fillna('unknown'))\n",
    "    translation_dict = {i:item for i, item in enumerate(label_model.classes_)}\n",
    "    return converted_train, converted_test, translation_dict\n",
    "\n",
    "# keep cats in one column\n",
    "def label_convert_cols(train, test):\n",
    "    cols = get_categoricals(train)\n",
    "    translation_dicts = {}\n",
    "    train_column_stacks = []\n",
    "    test_column_stacks = []\n",
    "    for item in cols:\n",
    "        train_column_labels, test_column_labels, translation_dict = encode_column(train[item], test[item])\n",
    "        train_column_stacks.append(train_column_labels)\n",
    "        test_column_stacks.append(test_column_labels)\n",
    "        translation_dicts[item] = translation_dict\n",
    "    train_arrays = np.vstack(train_column_stacks).T\n",
    "    test_arrays = np.vstack(test_column_stacks).T\n",
    "    return translation_dicts, train_arrays, test_arrays \n",
    "\n",
    "# converts all categorical columns into sparse format\n",
    "def convert_columns(train, test, cols):\n",
    "    \n",
    "    translation_dicts = {}\n",
    "    converted_train_arrays = []\n",
    "    converted_test_arrays = []\n",
    "    for column in cols:\n",
    "        converted_train, converted_test, translation_dict = label_convert(train[column], test[column])\n",
    "        translation_dicts[column] = translation_dict\n",
    "        converted_train_arrays.append(converted_train)\n",
    "        converted_test_arrays.append(converted_test)\n",
    "    stacked_train_arrays = np.hstack(converted_train_arrays)\n",
    "    stacked_test_arrays = np.hstack(converted_test_arrays)\n",
    "    return translation_dicts, stacked_train_arrays, stacked_test_arrays\n",
    "\n",
    "def process_categoricals(train, test):\n",
    "\n",
    "    columns = get_categoricals(train)\n",
    "    category_dicts, train_arrays, test_arrays = convert_columns(train, test, columns)\n",
    "    return category_dicts, train_arrays, test_arrays\n",
    "\n",
    "def combine_num_cats(train, test, wide=True):\n",
    "\n",
    "    # combines numerical and categorical columns    \n",
    "    \n",
    "    column_dict = {}\n",
    "    cols = get_categoricals(train)\n",
    "    num_cols = [item for item in train.columns if item not in cols]\n",
    "    for item in num_cols:\n",
    "        column_dict[item]='numerical_column'\n",
    "    if wide:    \n",
    "        category_dicts, train_arrays, test_arrays = process_categoricals(train, test)\n",
    "    else:\n",
    "        category_dicts, train_arrays, test_arrays = label_convert_cols(train, test)\n",
    "\n",
    "    # ** unpacks dict and allows extending dict\n",
    "    combined_dict = {**column_dict, **category_dicts}\n",
    "    train_matrix = np.hstack([train[num_cols].values, train_arrays])\n",
    "    test_matrix = np.hstack([test[num_cols].values, test_arrays])\n",
    "    \n",
    "    return combined_dict, train_matrix, test_matrix\n",
    "\n",
    "\n",
    "# needs fix for columns of 0 or 1 like sex# needs f \n",
    "def flatten_cols(column_dict):\n",
    "    \n",
    "    flattened_columns = {}\n",
    "    f = 0\n",
    "    \n",
    "    for item in column_dict.items():    \n",
    "        if type(item[1]) is dict:\n",
    "            if len(item[1]) > 2:\n",
    "                for name in item[1].values():\n",
    "                    flattened_columns.update({f:item[0] + '_' + name})\n",
    "                    f = f + 1\n",
    "            else:\n",
    "                flattened_columns.update({f:item[0]+ '_' + list(item[1].values())[1]})\n",
    "                f = f + 1\n",
    "        else:\n",
    "            flattened_columns.update({f:item[0]})\n",
    "            f = f + 1\n",
    "    return flattened_columns\n",
    "\n",
    "# def encode_column(column, fillna = False):\n",
    "    \n",
    "#     le = LabelEncoder()\n",
    "    \n",
    "#     # le does not work with nan, replace with string unknown\n",
    "#     column = column.fillna('unknown')\n",
    "#     le.fit(column)\n",
    "#     column_le = le.transform(column)\n",
    "    \n",
    "#     if fillna == False:\n",
    "#         # replace unknown with nan, nan must replace float value\n",
    "#         column_le = column_le.astype('float')\n",
    "#         idx = [i for i, value in enumerate(column_le) if column_le[i]=='unknown']\n",
    "#         column_le[idx] = np.nan\n",
    "\n",
    "#     zip_obj = zip(list(column), column_le)\n",
    "#     return column_le, dict(zip_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_id                  0\n",
       "name                      0\n",
       "item_condition_id         0\n",
       "category_name          1607\n",
       "brand_name           158259\n",
       "price                     0\n",
       "shipping                  0\n",
       "item_description          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name_miss = {name for name in data.brand_name if name not in test.brand_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brand_name_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_le, brand_dict = encode_column(data.brand_name, test.brand_name, fillna = False)\n",
    "category_le, category_dict = encode_column(data.category_name, test.category_name, fillna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['brands_le'] = brands_le\n",
    "data['category_le'] = category_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data[['item_condition_id', 'category_le', 'brands_le', 'shipping']].as_matrix()\n",
    "data_y = data.price.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?xgb.DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(data_x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_fix(array):\n",
    "    \n",
    "    tmp = np.zeros(len(array))\n",
    "    \n",
    "    for i, number in enumerate(array):\n",
    "        if not np.any(number):\n",
    "            tmp[i] = 0\n",
    "        else:\n",
    "            tmp[i] = np.log(number)\n",
    "    return tmp        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_ln = log_fix(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y_ln[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataing data splitting function\n",
    "# def data_split(data, proportion = .2):\n",
    "#     cutoff = round(len(data) * (1-proportion))\n",
    "#     return data[0:cutoff], data[cutoff:]\n",
    "\n",
    "train_x, val_x = train_test_split(data_x, data_y, test_size = .3, random_state = 42)\n",
    "train_y, val_y = data_split(data_x, data_y_ln, test_size = .3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = xgb.DMatrix(train_x, train_y)\n",
    "train_ln = xgb.DMatrix(train_x, train_y_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(~np.isnan(data_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'min_child_weight': 20, 'max_depth': 7,\n",
    "            'subsample': 0.91, 'lambda': 2.01, 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'eval_metric': 'rmse', 'objective': 'reg:linear','tree_method': 'auto'}\n",
    "\n",
    "model = xgb.data(params, ddata, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_score(importance_type='gain').values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=list(model.get_score(importance_type='gain').keys()), y=list(model.get_score(importance_type='gain').values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={'a':[2,3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ln=xgb.data(params, ddata_ln, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??xgb.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(data_x[0:10000])\n",
    "true = data_y[0:10000].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear non-transformed\n",
    "print(mean_squared_error(val_data_y, model.predict(val_data_x)))\n",
    "\n",
    "print(mean_squared_error(true, np.exp(model_ln.predict(dtest))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(dtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
